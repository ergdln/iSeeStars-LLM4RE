#!/usr/bin/env python3
"""
Servidor Flask para interface web interativa do I See Stars
"""

import os
import json
import uuid
from flask import Flask, render_template, request, jsonify, session
# N√£o importar OpenAI aqui - ser√° importado dentro da fun√ß√£o get_openai_client()
# para evitar problemas de compatibilidade e conflitos de vers√£o

app = Flask(__name__)
app.secret_key = os.urandom(24)

# Inicializar cliente OpenAI
def get_openai_client():
    """
    Cria e retorna um cliente OpenAI.
    Tenta m√∫ltiplas abordagens para garantir compatibilidade.
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY n√£o configurada")
    
    # Importar OpenAI dentro da fun√ß√£o para evitar problemas de importa√ß√£o
    try:
        from openai import OpenAI as OpenAIClient
    except ImportError as e:
        raise ValueError(f"Erro ao importar biblioteca OpenAI: {e}")
    
    # Configurar timeout para httpx (usado internamente pelo OpenAI)
    timeout_config = None
    try:
        import httpx
        timeout_config = httpx.Timeout(60.0, connect=10.0)  # 60s total, 10s para conectar
    except ImportError:
        pass  # httpx pode n√£o estar dispon√≠vel, usar sem timeout
    
    # Tentar m√∫ltiplas abordagens
    approaches = []
    if timeout_config:
        approaches.extend([
            # Abordagem 1: Usar vari√°vel de ambiente com timeout
            lambda: OpenAIClient(timeout=timeout_config),
            # Abordagem 2: Passar api_key explicitamente com timeout
            lambda: OpenAIClient(api_key=api_key, timeout=timeout_config),
        ])
    # Fallbacks sem timeout
    approaches.extend([
        # Abordagem 3: Sem timeout
        lambda: OpenAIClient(),
        # Abordagem 4: Com api_key sem timeout
        lambda: OpenAIClient(api_key=api_key),
    ])
    
    last_error = None
    for i, approach in enumerate(approaches, 1):
        try:
            # Garantir que a vari√°vel de ambiente est√° configurada
            os.environ['OPENAI_API_KEY'] = api_key
            client = approach()
            # Testar se o cliente funciona fazendo uma chamada simples (n√£o vamos realmente chamar)
            return client
        except (TypeError, ValueError) as e:
            error_msg = str(e)
            last_error = e
            # Se o erro mencionar 'proxies', pode ser um problema de vers√£o
            if 'proxies' in error_msg.lower() and i < len(approaches):
                # Tentar pr√≥xima abordagem
                continue
            elif 'proxies' in error_msg.lower():
                # √öltima tentativa: verificar vers√£o e sugerir atualiza√ß√£o
                import openai
                raise ValueError(
                    f"Erro de compatibilidade com biblioteca OpenAI. "
                    f"Vers√£o instalada: {openai.__version__}. "
                    f"O erro 'proxies' indica poss√≠vel incompatibilidade. "
                    f"Tente executar: pip install --upgrade --force-reinstall openai httpx. "
                    f"Erro original: {error_msg}"
                )
            else:
                # Outro tipo de erro, propagar
                raise ValueError(f"Erro ao inicializar cliente OpenAI: {error_msg}")
        except Exception as e:
            last_error = e
            if i < len(approaches):
                continue
            raise ValueError(f"Erro ao inicializar cliente OpenAI: {str(e)}")
    
    # Se chegou aqui, todas as abordagens falharam
    if last_error:
        raise ValueError(f"Erro ao inicializar cliente OpenAI ap√≥s tentar m√∫ltiplas abordagens: {str(last_error)}")
    raise ValueError("Erro desconhecido ao inicializar cliente OpenAI")

def ler_arquivo(caminho):
    """L√™ um arquivo e retorna o conte√∫do"""
    try:
        with open(caminho, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return None

def corrigir_json(dados):
    """Corrige JSON adicionando campos obrigat√≥rios e corrigindo erros"""
    import uuid as uuid_lib
    
    def adicionar_custom_properties(obj):
        if 'customProperties' not in obj:
            obj['customProperties'] = {"Description": ""}
        return obj
    
    def converter_ids_para_uuid(obj, id_map=None):
        if id_map is None:
            id_map = {}
        
        if isinstance(obj, dict):
            if 'id' in obj:
                old_id = obj['id']
                if not (isinstance(old_id, str) and len(old_id) == 36 and old_id.count('-') == 4):
                    new_id = str(uuid_lib.uuid4())
                    id_map[old_id] = new_id
                    obj['id'] = new_id
                else:
                    id_map[old_id] = old_id
            
            if 'type' in obj and any(t in str(obj.get('type', '')) for t in ['istar.Actor', 'istar.Agent', 'istar.Role', 'istar.Goal', 'istar.Task', 'istar.Quality', 'istar.Resource']):
                obj = adicionar_custom_properties(obj)
            
            for key, value in obj.items():
                if key == 'source' or key == 'target':
                    if value in id_map:
                        obj[key] = id_map[value]
                else:
                    obj[key] = converter_ids_para_uuid(value, id_map)
        elif isinstance(obj, list):
            return [converter_ids_para_uuid(item, id_map) for item in obj]
        
        return obj
    
    dados_corrigidos = converter_ids_para_uuid(dados)
    
    # Corrigir tipos de dependencies
    if 'dependencies' in dados_corrigidos:
        for dep in dados_corrigidos['dependencies']:
            if dep.get('type') == 'istar.DependencyLink':
                dep['type'] = 'istar.Goal'
    
    # Coletar IDs de atores e nodes
    ator_ids = set()
    node_ids = set()
    
    if 'actors' in dados_corrigidos:
        for actor in dados_corrigidos['actors']:
            ator_ids.add(actor.get('id'))
            for node in actor.get('nodes', []):
                node_ids.add(node.get('id'))
    
    # Remover links que conectam atores diretamente
    if 'links' in dados_corrigidos:
        links_validos = []
        for link in dados_corrigidos['links']:
            source = link.get('source')
            target = link.get('target')
            if source in ator_ids or target in ator_ids:
                continue
            links_validos.append(link)
        dados_corrigidos['links'] = links_validos
    
    # Garantir campos obrigat√≥rios
    if 'tool' not in dados_corrigidos:
        dados_corrigidos['tool'] = 'pistar.2.0.0'
    elif dados_corrigidos['tool'] not in ['pistar.2.0.0', 'pistar.2.1.0']:
        dados_corrigidos['tool'] = 'pistar.2.0.0'
    
    if 'istar' not in dados_corrigidos or dados_corrigidos['istar'] != '2.0':
        dados_corrigidos['istar'] = '2.0'
    
    if 'orphans' not in dados_corrigidos:
        dados_corrigidos['orphans'] = []
    
    if 'display' not in dados_corrigidos:
        dados_corrigidos['display'] = {}
    
    # Garantir que display tenha informa√ß√µes b√°sicas se necess√°rio
    if not dados_corrigidos['display'] and 'actors' in dados_corrigidos:
        # Adicionar informa√ß√µes b√°sicas de display para atores (opcional)
        for actor in dados_corrigidos['actors']:
            actor_id = actor.get('id')
            if actor_id:
                dados_corrigidos['display'][actor_id] = {'collapsed': False}
    
    if 'diagram' not in dados_corrigidos:
        dados_corrigidos['diagram'] = {
            "width": 1700,
            "height": 1300,
            "name": "",
            "customProperties": {}
        }
    elif 'customProperties' not in dados_corrigidos['diagram']:
        dados_corrigidos['diagram']['customProperties'] = {}
    
    if 'saveDate' not in dados_corrigidos:
        dados_corrigidos['saveDate'] = ""
    
    return dados_corrigidos

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('index.html')

@app.route('/api/start', methods=['POST'])
def start_session():
    """Inicia uma nova sess√£o de elicita√ß√£o"""
    data = request.json
    cenario = data.get('cenario', '').strip()
    
    if not cenario:
        return jsonify({'error': 'Cen√°rio n√£o pode estar vazio'}), 400
    
    # Inicializar sess√£o
    session_id = str(uuid.uuid4())
    session['session_id'] = session_id
    session['cenario'] = cenario
    session['respostas'] = []
    session['perguntas'] = []
    session['conversa'] = []  # Hist√≥rico da conversa
    session['num_perguntas'] = 0  # Contador de perguntas
    session['estado'] = 'conversando'
    
    return jsonify({
        'session_id': session_id,
        'status': 'success'
    })

@app.route('/api/ask-question', methods=['POST'])
def ask_question():
    """Gera a pr√≥xima pergunta baseada no contexto da conversa"""
    try:
        print(f"\n{'='*80}")
        print("API /api/ask-question chamada")
        print(f"Session ID: {session.get('session_id', 'N/A')}")
        print(f"{'='*80}\n")
        
        cenario = session.get('cenario')
        conversa = session.get('conversa', [])
        num_perguntas = session.get('num_perguntas', 0)
        
        print(f"Cen√°rio encontrado: {cenario is not None}")
        print(f"Tamanho da conversa: {len(conversa)}")
        print(f"N√∫mero de perguntas: {num_perguntas}")
        
        if not cenario:
            print("ERRO: Cen√°rio n√£o encontrado na sess√£o")
            return jsonify({'error': 'Cen√°rio n√£o encontrado. Por favor, inicie uma nova sess√£o.'}), 400
        
        # Limite de 5 perguntas - se j√° atingiu, for√ßar gera√ß√£o
        if num_perguntas >= 5:
            session['estado'] = 'pronto_para_gerar'
            return jsonify({
                'pergunta': None,
                'mensagem': 'Limite de 5 perguntas atingido. Tenho informa√ß√µes suficientes para gerar o modelo.',
                'pronto_para_gerar': True,
                'status': 'success'
            })
        
        # Carregar prompt conversacional (vers√£o compacta para melhor performance)
        prompt_template = ler_arquivo('prompts/conversational_elicitation_compact.txt')
        if not prompt_template:
            # Fallback para vers√£o completa se compacta n√£o existir
            prompt_template = ler_arquivo('prompts/conversational_elicitation.txt')
        if not prompt_template:
            return jsonify({'error': 'Prompt conversacional n√£o encontrado'}), 500
        
        # Construir hist√≥rico da conversa
        historico_texto = ""
        num_perguntas_feitas = sum(1 for item in conversa if item['tipo'] == 'pergunta')
        
        if conversa:
            for item in conversa:
                if item['tipo'] == 'pergunta':
                    historico_texto += f"IA: {item['texto']}\n\n"
                elif item['tipo'] == 'resposta':
                    historico_texto += f"Usu√°rio: {item['texto']}\n\n"
        else:
            historico_texto = "(Esta √© a primeira intera√ß√£o - ainda n√£o h√° hist√≥rico)"
        
        # Adicionar informa√ß√£o sobre n√∫mero de perguntas
        historico_texto += f"\n‚ö†Ô∏è IMPORTANTE: Voc√™ j√° fez {num_perguntas_feitas} pergunta(s). Voc√™ tem NO M√ÅXIMO 5 perguntas no total.\n"
        if num_perguntas_feitas >= 4:
            historico_texto += "‚ö†Ô∏è ATEN√á√ÉO: Esta deve ser sua √öLTIMA pergunta (5¬™). Ap√≥s esta, voc√™ DEVE indicar que est√° pronto para gerar o modelo.\n"
        
        # Substituir placeholders no prompt
        prompt_completo = prompt_template.replace('[INSERIR CEN√ÅRIO AQUI]', cenario)
        prompt_completo = prompt_completo.replace('[INSERIR HIST√ìRICO AQUI]', historico_texto)
        
        # Limitar tamanho do hist√≥rico para melhorar performance
        if len(historico_texto) > 2000:
            print(f"‚ö†Ô∏è Hist√≥rico grande ({len(historico_texto)} caracteres). Reduzindo para melhorar performance...")
            # Manter apenas √∫ltimas 2-3 intera√ß√µes (√∫ltimas perguntas e respostas)
            linhas_historico = historico_texto.split('\n\n')
            # Manter √∫ltimas 4-6 linhas (2-3 intera√ß√µes Q+A)
            historico_reduzido = '\n\n'.join(linhas_historico[-6:])
            historico_texto = historico_reduzido
            print(f"‚úì Hist√≥rico reduzido para {len(historico_texto)} caracteres")
        
        # Limitar tamanho do cen√°rio se muito grande
        if len(cenario) > 2000:
            print(f"‚ö†Ô∏è Cen√°rio muito grande ({len(cenario)} caracteres). Usando resumo...")
            cenario = cenario[:2000] + "..."
        
        # Reconstruir prompt com tamanhos limitados
        prompt_completo = prompt_template.replace('[INSERIR CEN√ÅRIO AQUI]', cenario)
        prompt_completo = prompt_completo.replace('[INSERIR HIST√ìRICO AQUI]', historico_texto)
        
        if len(prompt_completo) > 5000:
            print(f"‚ö†Ô∏è AVISO: Prompt ainda grande ({len(prompt_completo)} caracteres), mas dentro do limite aceit√°vel.")
        
        # Chamar API
        try:
            client = get_openai_client()
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Erro ao inicializar cliente OpenAI: {error_details}")
            return jsonify({'error': f'Erro ao inicializar cliente OpenAI: {str(e)}'}), 500
        
        model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
        
        print(f"\n{'='*80}")
        print(f"Gerando pergunta {num_perguntas_feitas + 1} de 5")
        print(f"Modelo: {model}")
        print(f"Tamanho do prompt: {len(prompt_completo)} caracteres")
        print(f"{'='*80}\n")
        
        try:
            # Verificar se API key est√° configurada
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                return jsonify({'error': 'OPENAI_API_KEY n√£o configurada. Configure a vari√°vel de ambiente antes de usar.'}), 500
            
            print(f"Chamando API OpenAI com modelo {model}...")
            print(f"Prompt tem {len(prompt_completo)} caracteres")
            
            # Chamar API com timeout atrav√©s do cliente httpx
            import time
            start_time = time.time()
            
            try:
                # Usar configura√ß√µes otimizadas para resposta mais r√°pida
                # max_tokens reduzido para acelerar (perguntas devem ser curtas)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": prompt_completo}
                    ],
                    temperature=0.7,
                    max_tokens=300  # Reduzido para 300 - perguntas devem ser curtas e diretas
                )
                elapsed_time = time.time() - start_time
                print(f"‚úì Resposta recebida em {elapsed_time:.2f} segundos")
                print(f"‚úì Tamanho da resposta: {len(response.choices[0].message.content)} caracteres")
            except Exception as api_error:
                elapsed_time = time.time() - start_time
                print(f"‚úó Erro ap√≥s {elapsed_time:.2f} segundos: {str(api_error)}")
                raise api_error
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"‚úó Erro ao chamar API OpenAI: {error_details}")
            
            error_msg = str(e)
            error_lower = error_msg.lower()
            
            if 'api key' in error_lower or 'authentication' in error_lower or 'invalid api key' in error_lower:
                return jsonify({'error': 'Erro de autentica√ß√£o. Verifique se OPENAI_API_KEY est√° configurada corretamente.'}), 500
            elif 'rate limit' in error_lower or 'quota' in error_lower:
                return jsonify({'error': 'Limite de taxa ou cota excedido. Aguarde alguns instantes e tente novamente.'}), 500
            elif 'timeout' in error_lower or 'timed out' in error_lower or 'connection' in error_lower:
                return jsonify({'error': 'Timeout na conex√£o com a API. Verifique sua conex√£o com a internet e tente novamente.'}), 500
            elif 'read timeout' in error_lower or 'request timeout' in error_lower:
                return jsonify({'error': 'A requisi√ß√£o demorou muito. O prompt pode estar muito grande. Tente com um cen√°rio mais curto.'}), 500
            else:
                # Retornar erro mais detalhado para debug
                return jsonify({
                    'error': f'Erro ao chamar API OpenAI: {error_msg[:200]}',
                    'details': 'Verifique os logs do servidor para mais informa√ß√µes.'
                }), 500
        
        # Verificar se a resposta est√° vazia
        if not response.choices or not response.choices[0].message.content:
            print("ERRO: Resposta vazia da API OpenAI")
            return jsonify({'error': 'A API OpenAI retornou uma resposta vazia. Tente novamente.'}), 500
        
        resposta_texto = response.choices[0].message.content.strip()
        
        if not resposta_texto:
            print("ERRO: Resposta vazia ap√≥s strip")
            return jsonify({'error': 'A API OpenAI retornou uma resposta vazia. Tente novamente.'}), 500
        
        print(f"Resposta processada: {resposta_texto[:100]}...")
        
        # Verificar se j√° atingiu o limite de perguntas
        num_perguntas = session.get('num_perguntas', 0)
        if num_perguntas >= 4:  # Se j√° fez 4 perguntas, esta ser√° a √∫ltima (5¬™)
            # For√ßar que est√° pronto ap√≥s a 5¬™ pergunta
            session['estado'] = 'pronto_para_gerar'
            # Ainda mostra a pergunta, mas marca como √∫ltima
            pass  # Continua para processar a pergunta
        
        # Verificar se a IA indicou que tem informa√ß√µes suficientes
        if 'informa√ß√µes suficientes' in resposta_texto.lower() or 'gerando modelo' in resposta_texto.lower():
            session['estado'] = 'pronto_para_gerar'
            return jsonify({
                'pergunta': None,
                'mensagem': resposta_texto,
                'pronto_para_gerar': True,
                'status': 'success'
            })
        
        # Extrair pergunta(s) da resposta
        perguntas = []
        linhas = resposta_texto.split('\n')
        for linha in linhas:
            linha = linha.strip()
            if linha and '?' in linha:
                # Limpar formata√ß√£o inicial
                linha_limpa = linha.lstrip('0123456789. ‚Ä¢-()[]').strip()
                if linha_limpa and len(linha_limpa) > 10:
                    perguntas.append(linha_limpa)
        
        # Se n√£o encontrou perguntas claras, usar a resposta completa
        if not perguntas:
            # Tentar encontrar a primeira frase com ?
            partes = resposta_texto.split('?')
            for parte in partes:
                parte_limpa = parte.strip()
                if parte_limpa and len(parte_limpa) > 20:
                    perguntas.append(parte_limpa + '?')
                    break
        
        # Limitar a 2 perguntas por vez
        perguntas = perguntas[:2]
        
        if not perguntas:
            # Se ainda n√£o encontrou, usar a resposta completa
            perguntas = [resposta_texto]
        
        # Adicionar pergunta(s) ao hist√≥rico
        for pergunta in perguntas:
            conversa.append({
                'tipo': 'pergunta',
                'texto': pergunta,
                'timestamp': str(uuid.uuid4())
            })
        
        # Incrementar contador de perguntas
        num_perguntas = session.get('num_perguntas', 0) + len(perguntas)
        session['num_perguntas'] = num_perguntas
        session['conversa'] = conversa
        
        # Se atingiu 5 perguntas, marcar como √∫ltima pergunta
        if num_perguntas >= 5:
            session['estado'] = 'aguardando_resposta'  # Ainda aguarda resposta da √∫ltima pergunta
            return jsonify({
                'perguntas': perguntas,
                'mensagem': resposta_texto,  # Mensagem original sem adicionar texto extra
                'pronto_para_gerar': False,  # Ainda n√£o est√° pronto, precisa da resposta
                'ultima_pergunta': True,  # Marca como √∫ltima pergunta
                'num_perguntas': num_perguntas,
                'status': 'success'
            })
        
        session['estado'] = 'aguardando_resposta'
        
        return jsonify({
            'perguntas': perguntas,
            'mensagem': resposta_texto,
            'pronto_para_gerar': False,
            'num_perguntas': num_perguntas,
            'status': 'success'
        })
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"Erro em ask_question: {error_details}")
        return jsonify({'error': f'Erro ao gerar pergunta: {str(e)}'}), 500

@app.route('/api/submit-answer', methods=['POST'])
def submit_answer():
    """Submete uma resposta do usu√°rio"""
    data = request.json
    resposta = data.get('resposta', '').strip()
    
    if not resposta:
        return jsonify({'error': 'Resposta n√£o pode estar vazia'}), 400
    
    # Adicionar resposta ao hist√≥rico da conversa
    conversa = session.get('conversa', [])
    conversa.append({
        'tipo': 'resposta',
        'texto': resposta,
        'timestamp': str(uuid.uuid4())
    })
    session['conversa'] = conversa
    
    # Tamb√©m adicionar √† lista de respostas (para compatibilidade)
    respostas = session.get('respostas', [])
    respostas.append({
        'resposta': resposta
    })
    session['respostas'] = respostas
    
    # Verificar se j√° atingiu 5 perguntas e todas foram respondidas
    num_perguntas = session.get('num_perguntas', 0)
    num_respostas = len([item for item in conversa if item['tipo'] == 'resposta'])
    
    if num_perguntas >= 5 and num_respostas >= 5:
        session['estado'] = 'pronto_para_gerar'
        return jsonify({
            'status': 'success',
            'pronto_para_gerar': True,
            'mensagem': 'Todas as perguntas foram respondidas. Voc√™ pode gerar o modelo agora.'
        })
    else:
        session['estado'] = 'conversando'
        return jsonify({'status': 'success'})

@app.route('/api/generate-model', methods=['POST'])
def generate_model():
    """Gera o modelo iStar 2.0 final"""
    try:
        cenario = session.get('cenario')
        respostas = session.get('respostas', [])
        
        if not cenario:
            return jsonify({'error': 'Cen√°rio n√£o encontrado'}), 400
        
        # Carregar prompt de gera√ß√£o final
        prompt_template = ler_arquivo('prompts/final_json_generation.txt')
        if not prompt_template:
            return jsonify({'error': 'Prompt de gera√ß√£o n√£o encontrado'}), 500
        
        # Construir contexto completo com cen√°rio e hist√≥rico da conversa
        contexto_cenario = cenario
        
        # Usar hist√≥rico da conversa se dispon√≠vel, sen√£o usar respostas antigas
        conversa = session.get('conversa', [])
        contexto_completo = ""
        
        # Construir contexto completo da conversa
        if conversa:
            contexto_completo = "=== HIST√ìRICO COMPLETO DA CONVERSA ===\n\n"
            for item in conversa:
                if item['tipo'] == 'pergunta':
                    contexto_completo += f"ü§ñ IA: {item['texto']}\n\n"
                elif item['tipo'] == 'resposta':
                    contexto_completo += f"üë§ USU√ÅRIO: {item['texto']}\n\n"
                elif item['tipo'] == 'system':
                    contexto_completo += f"‚ÑπÔ∏è {item['texto']}\n\n"
        elif respostas:
            # Fallback para formato antigo
            perguntas = session.get('perguntas', [])
            contexto_completo = "=== PERGUNTAS E RESPOSTAS ===\n\n"
            for i, resp in enumerate(respostas):
                pergunta_texto = perguntas[resp.get('pergunta_id', i)] if resp.get('pergunta_id') and resp['pergunta_id'] < len(perguntas) else f"Pergunta {i + 1}"
                contexto_completo += f"Q: {pergunta_texto}\nA: {resp.get('resposta', resp)}\n\n"
        
        # Substituir placeholders no prompt
        prompt_completo = prompt_template.replace('[CEN√ÅRIO ORIGINAL]', contexto_cenario)
        
        # Adicionar contexto completo da conversa de forma expl√≠cita
        if contexto_completo:
            # Primeiro, tentar substituir o placeholder espec√≠fico
            if 'HIST√ìRICO COMPLETO DA CONVERSA:\n[RESPOSTAS DO USU√ÅRIO]' in prompt_completo:
                prompt_completo = prompt_completo.replace(
                    'HIST√ìRICO COMPLETO DA CONVERSA:\n[RESPOSTAS DO USU√ÅRIO]',
                    f'HIST√ìRICO COMPLETO DA CONVERSA:\n{contexto_completo}'
                )
            elif '[RESPOSTAS DO USU√ÅRIO]' in prompt_completo:
                prompt_completo = prompt_completo.replace('[RESPOSTAS DO USU√ÅRIO]', contexto_completo)
            else:
                # Se n√£o encontrou placeholder, adicionar ap√≥s o cen√°rio
                prompt_completo = prompt_completo.replace(
                    'CEN√ÅRIO ORIGINAL:\n[CEN√ÅRIO ORIGINAL]',
                    f'CEN√ÅRIO ORIGINAL:\n{contexto_cenario}\n\nHIST√ìRICO COMPLETO DA CONVERSA:\n{contexto_completo}'
                )
            
            # Log para debug
            print(f"\n{'='*80}")
            print("CONTEXTO DA CONVERSA ENVIADO PARA GERA√á√ÉO:")
            print(f"{'='*80}")
            print(f"Tamanho total do contexto: {len(contexto_completo)} caracteres")
            print(f"N√∫mero de intera√ß√µes: {len([item for item in conversa if item['tipo'] in ['pergunta', 'resposta']])}")
            print(f"\nPrimeiros 800 caracteres do contexto:")
            print(contexto_completo[:800] + "..." if len(contexto_completo) > 800 else contexto_completo)
            print(f"\n√öltimos 300 caracteres do contexto:")
            print("..." + contexto_completo[-300:] if len(contexto_completo) > 300 else contexto_completo)
            print(f"{'='*80}\n")
        
        # Adicionar instru√ß√£o final expl√≠cita sobre usar todo o contexto
        if contexto_completo:
            prompt_completo += "\n\n" + "="*80 + "\n"
            prompt_completo += "INSTRU√á√ÉO FINAL CR√çTICA:\n"
            prompt_completo += "="*80 + "\n"
            prompt_completo += "Voc√™ recebeu uma conversa completa acima. O modelo JSON que voc√™ gerar DEVE:\n"
            prompt_completo += "1. Incluir TODAS as tarefas, recursos, objetivos e qualidades mencionados pelo usu√°rio\n"
            prompt_completo += "2. Usar os nomes e descri√ß√µes EXATAS fornecidas pelo usu√°rio\n"
            prompt_completo += "3. Criar links e dependencies baseados nas informa√ß√µes da conversa\n"
            prompt_completo += "4. N√ÉO ignorar ou simplificar informa√ß√µes detalhadas fornecidas\n"
            prompt_completo += "5. Ser completo e refletir fielmente toda a conversa\n"
            prompt_completo += "\n"
        
        # Garantir que o prompt tenha instru√ß√£o clara
        if 'Retorne APENAS o JSON' not in prompt_completo:
            prompt_completo += "\n\nIMPORTANTE: Retorne APENAS o JSON do modelo, sem explica√ß√µes ou texto adicional."
        
        # Chamar API
        try:
            client = get_openai_client()
        except Exception as e:
            return jsonify({'error': f'Erro ao inicializar cliente OpenAI: {str(e)}'}), 500
        
        model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt_completo}
                ],
                temperature=0.3,
                max_tokens=8000  # Aumentado para permitir modelos mais completos
            )
        except Exception as e:
            return jsonify({'error': f'Erro ao chamar API OpenAI: {str(e)}'}), 500
        
        resposta_texto = response.choices[0].message.content
        
        # Extrair JSON
        json_texto = resposta_texto
        if "```json" in resposta_texto:
            inicio = resposta_texto.find("```json") + 7
            fim = resposta_texto.find("```", inicio)
            json_texto = resposta_texto[inicio:fim].strip()
        elif "```" in resposta_texto:
            inicio = resposta_texto.find("```") + 3
            fim = resposta_texto.find("```", inicio)
            json_texto = resposta_texto[inicio:fim].strip()
        else:
            # Tentar encontrar JSON direto
            inicio = resposta_texto.find("{")
            fim = resposta_texto.rfind("}") + 1
            if inicio >= 0 and fim > inicio:
                json_texto = resposta_texto[inicio:fim]
        
        # Validar e corrigir JSON
        try:
            dados = json.loads(json_texto)
            dados_corrigidos = corrigir_json(dados)
            
            session['modelo_json'] = dados_corrigidos
            session['estado'] = 'modelo_gerado'
            
            return jsonify({
                'modelo': dados_corrigidos,
                'status': 'success'
            })
        except json.JSONDecodeError as e:
            return jsonify({
                'error': f'JSON inv√°lido: {str(e)}',
                'resposta_bruta': resposta_texto[:500]
            }), 400
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"Erro em generate_model: {error_details}")
        return jsonify({'error': f'Erro ao gerar modelo: {str(e)}'}), 500

@app.route('/api/download-model', methods=['GET'])
def download_model():
    """Baixa o modelo JSON gerado"""
    modelo = session.get('modelo_json')
    if not modelo:
        return jsonify({'error': 'Modelo n√£o encontrado'}), 404
    
    return jsonify(modelo)

if __name__ == '__main__':
    # Verificar se API key est√° configurada
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ö†Ô∏è  AVISO: OPENAI_API_KEY n√£o configurada!")
        print("   Execute: export OPENAI_API_KEY='sua-chave'")
    
    print("=" * 60)
    print("üî≠ I SEE STARS - Interface Web")
    print("=" * 60)
    print()
    print("üåê Servidor iniciando em: http://localhost:5000")
    print("üìù Certifique-se de que OPENAI_API_KEY est√° configurada")
    print()
    print("Pressione Ctrl+C para parar o servidor")
    print("=" * 60)
    print()
    
    app.run(debug=True, host='0.0.0.0', port=5001)

