# Plano de A√ß√£o Completo - I See Stars

## üìã √çndice

1. [Metodologia](#1-metodologia)
2. [Setup dos Cen√°rios](#2-setup-dos-cen√°rios)
3. [Processo de Prompting](#3-processo-de-prompting)
4. [M√©tricas de Avalia√ß√£o](#4-m√©tricas-de-avalia√ß√£o)
5. [Valida√ß√£o de JSON](#5-processo-para-validar-o-json)
6. [Compara√ß√£o Baseline vs Interativo](#6-processo-para-comparar-baseline-vs-interativo)
7. [Checklist de Reprodutibilidade](#7-checklist-de-reprodutibilidade)

---

## 1. Metodologia

### 1.1 Objetivo da Metodologia

Avaliar se uma abordagem interativa de elicita√ß√£o de requisitos (onde o LLM faz perguntas antes de gerar o modelo) produz modelos iStar 2.0 de melhor qualidade comparado a uma abordagem baseline (zero-shot).

### 1.2 Design Experimental

**Tipo**: Estudo comparativo experimental

**Vari√°veis Independentes**:
- Abordagem de prompting (baseline vs. interativa)
- Cen√°rios de requisitos (3-5 diferentes)
- Modelo LLM utilizado (ex: GPT-4, Claude)

**Vari√°veis Dependentes**:
- Completude do modelo
- Clareza do modelo
- Conformidade com iStar 2.0
- Qualidade das perguntas (apenas abordagem interativa)

**Controles**:
- Mesmos cen√°rios para ambas as abordagens
- Mesmos par√¢metros do LLM (temperatura, max_tokens)
- Mesmos avaliadores
- Mesmo processo de valida√ß√£o

### 1.3 Fases do Estudo

#### Fase 1: Prepara√ß√£o (Semana 1-2)
- **Objetivo**: Preparar todos os materiais necess√°rios
- **Atividades**:
  - Criar/validar cen√°rios de requisitos
  - Desenvolver prompts baseline e interativos
  - Criar modelos de refer√™ncia (gold standard)
  - Configurar ambiente de experimenta√ß√£o

#### Fase 2: Execu√ß√£o Baseline (Semana 3)
- **Objetivo**: Gerar modelos usando abordagem zero-shot
- **Atividades**:
  - Executar experimentos baseline para todos os cen√°rios
  - Validar e armazenar modelos gerados
  - Registrar logs e metadados

#### Fase 3: Execu√ß√£o Interativa (Semana 4-5)
- **Objetivo**: Gerar modelos usando abordagem interativa
- **Atividades**:
  - Executar processo interativo para todos os cen√°rios
  - Coletar perguntas e respostas
  - Gerar e validar modelos finais
  - Registrar logs e metadados

#### Fase 4: Avalia√ß√£o (Semana 6-7)
- **Objetivo**: Avaliar qualidade dos modelos gerados
- **Atividades**:
  - Calcular m√©tricas quantitativas
  - Coletar avalia√ß√µes de especialistas
  - Comparar com modelos de refer√™ncia
  - Analisar qualidade das perguntas

#### Fase 5: An√°lise e Relat√≥rio (Semana 8)
- **Objetivo**: Consolidar resultados e escrever relat√≥rio
- **Atividades**:
  - An√°lise estat√≠stica comparativa
  - Interpreta√ß√£o de resultados
  - Documenta√ß√£o completa
  - Prepara√ß√£o de visualiza√ß√µes

### 1.4 Crit√©rios de Sucesso

- **Completude**: Modelos interativos devem ter ‚â• 10% mais elementos identificados
- **Conformidade**: ‚â• 95% dos modelos devem passar valida√ß√£o estrutural
- **Qualidade**: Especialistas devem preferir modelos interativos em ‚â• 70% dos casos

---

## 2. Setup dos Cen√°rios

### 2.1 Objetivo

Criar e preparar cen√°rios de requisitos em linguagem natural que sejam representativos, tenham ambiguidades intencionais apropriadas e variem em dom√≠nio e complexidade.

### 2.2 Passos para Cria√ß√£o de Cen√°rios

#### Passo 2.2.1: Sele√ß√£o de Dom√≠nios
- [ ] Identificar 3-5 dom√≠nios diferentes
  - Exemplos: transporte, educa√ß√£o, sa√∫de, com√©rcio, social
- [ ] Garantir que dom√≠nios sejam familiares aos avaliadores
- [ ] Documentar escolha dos dom√≠nios

#### Passo 2.2.2: Reda√ß√£o dos Cen√°rios
- [ ] Escrever descri√ß√£o do sistema (200-500 palavras)
- [ ] Incluir requisitos informais em linguagem natural
- [ ] **Inserir ambiguidades intencionais**:
  - Atores n√£o explicitamente definidos
  - Metas impl√≠citas ou vagas
  - Processos incompletos
  - Depend√™ncias n√£o claras
  - Crit√©rios de qualidade n√£o especificados
- [ ] Garantir que cen√°rio seja compreens√≠vel apesar das ambiguidades
- [ ] Revisar clareza b√°sica do texto

#### Passo 2.2.3: Documenta√ß√£o de Metadados
- [ ] Criar arquivo `scenarios_metadata.json` com:
  ```json
  {
    "scenario_id": "scenario_001",
    "name": "Taxi App System",
    "domain": "transportation",
    "complexity": "medium",
    "word_count": 250,
    "intentional_ambiguities": [
      "Payment method not specified",
      "Rating criteria unclear"
    ],
    "expected_actors": ["passenger", "driver", "system"],
    "expected_goals": 5,
    "created_at": "2024-12-01"
  }
  ```
- [ ] Salvar cen√°rio em `/scenarios/scenario_{id}_{name}.md`
- [ ] Atualizar √≠ndice de cen√°rios

#### Passo 2.2.4: Valida√ß√£o dos Cen√°rios
- [ ] Revisar com pelo menos 2 especialistas em RE
- [ ] Verificar que ambiguidades s√£o apropriadas (n√£o excessivas)
- [ ] Confirmar que cen√°rio √© representativo
- [ ] Validar tamanho e complexidade
- [ ] Incorporar feedback e revisar

#### Passo 2.2.5: Cria√ß√£o de Modelos de Refer√™ncia
- [ ] Para cada cen√°rio, criar modelo iStar 2.0 manualmente
- [ ] Incluir todos os elementos esperados (atores, metas, softgoals, tarefas, depend√™ncias)
- [ ] Validar conformidade com iStar 2.0
- [ ] Salvar em `/models/reference/scenario_{id}_gold_standard.json`
- [ ] Documentar decis√µes de modelagem

### 2.3 Checklist de Qualidade dos Cen√°rios

- [ ] Cen√°rio tem 200-500 palavras
- [ ] Cont√©m 3-5 ambiguidades intencionais claras
- [ ] √â compreens√≠vel para leitores n√£o-especialistas
- [ ] Representa um sistema realista ou plaus√≠vel
- [ ] Varia em complexidade (simples, m√©dio, complexo)
- [ ] Metadados completos documentados
- [ ] Modelo de refer√™ncia criado e validado
- [ ] Revisado por especialistas

---

## 3. Processo de Prompting

### 3.1 Abordagem Baseline (Zero-Shot)

#### 3.1.1 Objetivo
Gerar modelo iStar 2.0 diretamente a partir do cen√°rio, sem intera√ß√£o pr√©via.

#### 3.1.2 Estrutura do Prompt Baseline

**Componentes obrigat√≥rios**:
1. **Contexto sobre iStar 2.0**
2. **Explica√ß√£o do dom√≠nio** (se aplic√°vel)
3. **Cen√°rio de requisitos**
4. **Instru√ß√µes de gera√ß√£o**
5. **Especifica√ß√£o do formato JSON**
6. **Constraints e regras**

#### 3.1.3 Passos para Execu√ß√£o Baseline

**Passo 3.1.3.1: Preparar Prompt**
- [ ] Carregar template base de prompt (`/prompts/baseline/zero_shot_template.md`)
- [ ] Inserir explica√ß√£o sobre iStar 2.0
- [ ] Inserir cen√°rio de requisitos
- [ ] Inserir especifica√ß√£o do formato JSON esperado
- [ ] Adicionar constraints (ex: "N√£o invente elementos n√£o mencionados")
- [ ] Validar prompt completo

**Passo 3.1.3.2: Configurar Par√¢metros do LLM**
- [ ] Definir modelo LLM (ex: `gpt-4`, `claude-3-opus`)
- [ ] Configurar temperatura: `0.3` (baixa para consist√™ncia)
- [ ] Configurar max_tokens: `2000-4000` (dependendo do modelo)
- [ ] Salvar configura√ß√£o em `/experiments/config/baseline_config.json`

**Passo 3.1.3.3: Executar Gera√ß√£o**
- [ ] Enviar prompt para API do LLM
- [ ] Registrar timestamp e metadados
- [ ] Capturar resposta completa
- [ ] Salvar log em `/experiments/logs/baseline_{scenario_id}_{timestamp}.log`

**Passo 3.1.3.4: Processar Resposta**
- [ ] Extrair JSON da resposta (pode estar em code blocks)
- [ ] Validar estrutura JSON b√°sica
- [ ] Se inv√°lido, tentar parsing/limpeza
- [ ] Salvar modelo em `/models/baseline/scenario_{id}_baseline_{timestamp}.json`

**Passo 3.1.3.5: Validar e Registrar**
- [ ] Executar valida√ß√£o de JSON (ver Se√ß√£o 5)
- [ ] Registrar resultado da valida√ß√£o
- [ ] Adicionar metadados ao modelo:
  ```json
  {
    "metadata": {
      "generated_at": "2024-12-01T10:00:00Z",
      "approach": "baseline",
      "scenario_id": "scenario_001",
      "llm_model": "gpt-4",
      "temperature": 0.3,
      "prompt_version": "v1.0",
      "validation_status": "valid|invalid|partial"
    }
  }
  ```

#### 3.1.4 Template de Prompt Baseline

```
Voc√™ √© um especialista em Engenharia de Requisitos e nota√ß√£o iStar 2.0.

CONTEXTO SOBRE iSTAR 2.0:
[iStar 2.0 √© uma nota√ß√£o para modelagem de requisitos orientada a objetivos...]

ELEMENTOS PRINCIPAIS:
- Actors: Agentes do sistema (humanos, sistemas, organiza√ß√µes)
- Goals: Objetivos que atores desejam alcan√ßar
- Softgoals: Objetivos qualitativos (ex: "seguran√ßa", "usabilidade")
- Tasks: Atividades espec√≠ficas para alcan√ßar goals
- Dependencies: Rela√ß√µes de depend√™ncia entre atores

CEN√ÅRIO DE REQUISITOS:
[Inserir cen√°rio aqui]

INSTRU√á√ïES:
1. Analise o cen√°rio acima
2. Identifique todos os atores, goals, softgoals, tasks e dependencies
3. Gere um modelo iStar 2.0 completo em formato JSON

FORMATO DE SA√çDA (JSON):
IMPORTANTE: Use EXATAMENTE a estrutura JSON do Pistar 2.0.0. Consulte ISTAR_2_0_JSON_STRUCTURE.md para a estrutura completa.

Estrutura b√°sica:
{
  "actors": [
    {
      "id": "uuid",
      "text": "Nome do Ator",
      "type": "istar.Agent | istar.Role | istar.Actor",
      "x": 0,
      "y": 0,
      "nodes": [
        {
          "id": "uuid",
          "text": "Nome",
          "type": "istar.Goal | istar.Task | istar.Quality | istar.Resource",
          "x": 0,
          "y": 0
        }
      ]
    }
  ],
  "orphans": [],
  "dependencies": [
    {
      "id": "uuid",
      "text": "Nome",
      "type": "istar.Goal | istar.Task | istar.Quality | istar.Resource",
      "x": 0,
      "y": 0,
      "source": "id-ator-depender",
      "target": "id-ator-dependee"
    }
  ],
  "links": [
    {
      "id": "uuid",
      "type": "istar.AndRefinementLink | istar.OrRefinementLink | ...",
      "source": "id-origem",
      "target": "id-destino",
      "label": ""
    }
  ],
  "display": {},
  "tool": "pistar.2.0.0",
  "istar": "2.0",
  "saveDate": "",
  "diagram": {
    "width": 1700,
    "height": 1300,
    "name": "",
    "customProperties": {}
  }
}

CONSTRAINTS:
- Use apenas elementos mencionados no cen√°rio
- N√£o invente detalhes n√£o especificados
- Siga estritamente o formato JSON acima
- Todos os IDs devem ser √∫nicos
```

### 3.2 Abordagem Interativa (Elicita√ß√£o Guiada)

#### 3.2.1 Objetivo
Gerar modelo iStar 2.0 atrav√©s de um processo em duas fases: primeiro o LLM gera perguntas de clarifica√ß√£o, depois gera o modelo usando as respostas.

#### 3.2.2 Estrutura do Processo Interativo

**Fase 1: Gera√ß√£o de Perguntas**
- LLM analisa cen√°rio
- Gera 5-8 perguntas de clarifica√ß√£o
- Perguntas focam em: atores, goals, softgoals, tasks, dependencies

**Fase 2: Gera√ß√£o do Modelo**
- LLM recebe: cen√°rio original + perguntas + respostas
- Gera modelo iStar 2.0 final

#### 3.2.3 Passos para Execu√ß√£o Interativa

**FASE 1: Gera√ß√£o de Perguntas**

**Passo 3.2.3.1: Preparar Prompt de Perguntas**
- [ ] Carregar template (`/prompts/interactive/question_generation_template.md`)
- [ ] Inserir explica√ß√£o sobre iStar 2.0
- [ ] Inserir cen√°rio de requisitos
- [ ] Inserir instru√ß√µes para gerar perguntas
- [ ] Especificar n√∫mero de perguntas (5-8)
- [ ] Especificar categorias (atores, goals, softgoals, tasks, dependencies)
- [ ] Validar prompt completo

**Passo 3.2.3.2: Configurar Par√¢metros do LLM (Fase 1)**
- [ ] Definir modelo LLM
- [ ] Configurar temperatura: `0.5` (mais criativo para perguntas)
- [ ] Configurar max_tokens: `1000-1500`
- [ ] Salvar configura√ß√£o

**Passo 3.2.3.3: Executar Gera√ß√£o de Perguntas**
- [ ] Enviar prompt para API do LLM
- [ ] Registrar timestamp
- [ ] Capturar resposta com perguntas
- [ ] Salvar log

**Passo 3.2.3.4: Processar e Validar Perguntas**
- [ ] Extrair lista de perguntas da resposta
- [ ] Validar formato (lista numerada ou JSON)
- [ ] Verificar que h√° 5-8 perguntas
- [ ] Categorizar perguntas (atores, goals, etc.)
- [ ] Salvar perguntas em `/experiments/interactive/questions_{scenario_id}_{timestamp}.json`

**Passo 3.2.3.5: Apresentar Perguntas ao Usu√°rio**
- [ ] Usar interface (`/interface`) ou processo manual
- [ ] Exibir cada pergunta numerada
- [ ] Coletar resposta do usu√°rio
- [ ] Validar que respostas n√£o est√£o vazias
- [ ] Salvar perguntas e respostas pareadas

**FASE 2: Gera√ß√£o do Modelo**

**Passo 3.2.3.6: Preparar Prompt de Gera√ß√£o Final**
- [ ] Carregar template (`/prompts/interactive/model_generation_template.md`)
- [ ] Inserir explica√ß√£o sobre iStar 2.0
- [ ] Inserir cen√°rio original
- [ ] Inserir perguntas geradas
- [ ] Inserir respostas do usu√°rio
- [ ] Inserir instru√ß√µes para gerar modelo
- [ ] Inserir especifica√ß√£o do formato JSON
- [ ] Validar prompt completo

**Passo 3.2.3.7: Configurar Par√¢metros do LLM (Fase 2)**
- [ ] Definir modelo LLM (mesmo da Fase 1)
- [ ] Configurar temperatura: `0.3` (consist√™ncia)
- [ ] Configurar max_tokens: `2000-4000`
- [ ] Salvar configura√ß√£o

**Passo 3.2.3.8: Executar Gera√ß√£o do Modelo**
- [ ] Enviar prompt completo para API do LLM
- [ ] Registrar timestamp
- [ ] Capturar resposta completa
- [ ] Salvar log em `/experiments/logs/interactive_{scenario_id}_{timestamp}.log`

**Passo 3.2.3.9: Processar Resposta**
- [ ] Extrair JSON da resposta
- [ ] Validar estrutura JSON b√°sica
- [ ] Se inv√°lido, tentar parsing/limpeza
- [ ] Salvar modelo em `/models/interactive/scenario_{id}_interactive_{timestamp}.json`

**Passo 3.2.3.10: Validar e Registrar**
- [ ] Executar valida√ß√£o de JSON (ver Se√ß√£o 5)
- [ ] Registrar resultado da valida√ß√£o
- [ ] Adicionar metadados ao modelo incluindo:
  - Perguntas geradas
  - Respostas fornecidas
  - Timestamp de cada fase

#### 3.2.4 Template de Prompt - Fase 1 (Perguntas)

```
Voc√™ √© um engenheiro de requisitos especializado em iStar 2.0.

CONTEXTO SOBRE iSTAR 2.0:
[Explica√ß√£o sobre iStar 2.0...]

CEN√ÅRIO DE REQUISITOS:
[Inserir cen√°rio aqui]

TAREFA:
Analise o cen√°rio acima e identifique √°reas que precisam de clarifica√ß√£o para criar um modelo iStar 2.0 completo e preciso.

Gere 5-8 perguntas de clarifica√ß√£o focadas em:
- Atores: Quem s√£o os principais atores? H√° atores impl√≠citos?
- Goals: Quais s√£o os objetivos principais de cada ator?
- Softgoals: Quais s√£o os crit√©rios de qualidade (ex: seguran√ßa, usabilidade)?
- Tasks: Quais s√£o as tarefas espec√≠ficas para alcan√ßar os goals?
- Dependencies: Quais s√£o as depend√™ncias entre atores?

FORMATO DE SA√çDA:
Liste as perguntas numeradas, uma por linha. Cada pergunta deve ser:
- Espec√≠fica e acion√°vel
- Focada em um aspecto do modelo iStar
- Capaz de ser respondida de forma concisa
```

#### 3.2.5 Template de Prompt - Fase 2 (Modelo)

```
Voc√™ √© um especialista em Engenharia de Requisitos e nota√ß√£o iStar 2.0.

CONTEXTO SOBRE iSTAR 2.0:
[Explica√ß√£o sobre iStar 2.0...]

CEN√ÅRIO DE REQUISITOS ORIGINAL:
[Inserir cen√°rio original]

PERGUNTAS DE CLARIFICA√á√ÉO E RESPOSTAS:
Q1: [Pergunta 1]
A1: [Resposta 1]

Q2: [Pergunta 2]
A2: [Resposta 2]

[... mais perguntas e respostas ...]

INSTRU√á√ïES:
1. Use o cen√°rio original E as respostas √†s perguntas acima
2. Gere um modelo iStar 2.0 completo e preciso
3. Incorpore as informa√ß√µes das respostas no modelo
4. Siga estritamente o formato JSON do Pistar 2.0.0 (consulte ISTAR_2_0_JSON_STRUCTURE.md)

FORMATO DE SA√çDA (JSON):
IMPORTANTE: Use EXATAMENTE a estrutura JSON do Pistar 2.0.0.

{
  "actors": [
    {
      "id": "uuid",
      "text": "Nome",
      "type": "istar.Agent | istar.Role | istar.Actor",
      "x": 0,
      "y": 0,
      "nodes": [
        {
          "id": "uuid",
          "text": "Nome",
          "type": "istar.Goal | istar.Task | istar.Quality | istar.Resource",
          "x": 0,
          "y": 0
        }
      ]
    }
  ],
  "orphans": [],
  "dependencies": [
    {
      "id": "uuid",
      "text": "Nome",
      "type": "istar.Goal | istar.Task | istar.Quality | istar.Resource",
      "x": 0,
      "y": 0,
      "source": "id-ator-depender",
      "target": "id-ator-dependee"
    }
  ],
  "links": [],
  "display": {},
  "tool": "pistar.2.0.0",
  "istar": "2.0",
  "saveDate": "",
  "diagram": {
    "width": 1700,
    "height": 1300,
    "name": "",
    "customProperties": {}
  }
}

CONSTRAINTS:
- Use informa√ß√µes do cen√°rio E das respostas
- N√£o invente elementos n√£o mencionados
- Siga estritamente o formato JSON acima (Pistar 2.0.0)
- Todos os IDs devem ser √∫nicos (use UUIDs)
- source/target sempre referem a IDs existentes
- tool deve ser exatamente "pistar.2.0.0"
- istar deve ser exatamente "2.0"
```

### 3.3 Versionamento de Prompts

- [ ] Cada vers√£o de prompt deve ser salva com vers√£o (v1.0, v1.1, etc.)
- [ ] Documentar mudan√ßas entre vers√µes
- [ ] Manter hist√≥rico de efetividade
- [ ] Usar mesma vers√£o para todos os cen√°rios em um experimento

---

## 4. M√©tricas de Avalia√ß√£o

### 4.1 Objetivo

Avaliar quantitativamente e qualitativamente a qualidade dos modelos gerados, comparando abordagens baseline e interativa.

### 4.2 M√©trica 1: Completude

#### 4.2.1 Defini√ß√£o
Mede a extens√£o em que o modelo captura todos os elementos esperados do cen√°rio.

#### 4.2.2 Elementos Avaliados
- **Atores (Actors)**: Agentes identificados
- **Metas (Goals)**: Objetivos identificados
- **Softgoals**: Crit√©rios de qualidade identificados
- **Tarefas (Tasks)**: Atividades identificadas
- **Depend√™ncias (Dependencies)**: Rela√ß√µes identificadas

#### 4.2.3 C√°lculo de Completude

**Passo 4.2.3.1: Preparar Dados**
- [ ] Carregar modelo gerado
- [ ] Carregar modelo de refer√™ncia (gold standard)
- [ ] Extrair listas de elementos de cada modelo

**Passo 4.2.3.2: Calcular Completude por Elemento**
Para cada tipo de elemento (atores, goals, etc.):

```
Completude = (Elementos Corretos Identificados / Elementos Esperados) √ó 100%
```

**Elementos Corretos**: Elementos do modelo gerado que correspondem a elementos do modelo de refer√™ncia (match por nome/sem√¢ntica).

**Passo 4.2.3.3: Calcular Completude Geral**
```
Completude Geral = M√©dia das completudes de cada tipo de elemento
```

**Passo 4.2.3.4: Implementar Script**
- [ ] Criar script `/evaluation/metrics/completeness_calculator.py`
- [ ] Fun√ß√£o para matching de elementos (exato + sem√¢ntico)
- [ ] Fun√ß√£o para calcular completude por tipo
- [ ] Fun√ß√£o para calcular completude geral
- [ ] Salvar resultados em `/evaluation/results/completeness_{scenario_id}.json`

#### 4.2.4 Exemplo de C√°lculo

```
Modelo de Refer√™ncia:
- Atores: 3 (passenger, driver, system)
- Goals: 5
- Softgoals: 2
- Tasks: 4
- Dependencies: 3

Modelo Gerado:
- Atores: 2 (passenger, driver) ‚Üí Completude: 2/3 = 66.7%
- Goals: 4 ‚Üí Completude: 4/5 = 80%
- Softgoals: 1 ‚Üí Completude: 1/2 = 50%
- Tasks: 3 ‚Üí Completude: 3/4 = 75%
- Dependencies: 2 ‚Üí Completude: 2/3 = 66.7%

Completude Geral: (66.7 + 80 + 50 + 75 + 66.7) / 5 = 67.7%
```

### 4.3 M√©trica 2: Clareza

#### 4.3.1 Defini√ß√£o
Mede qu√£o claros e bem definidos s√£o os elementos do modelo.

#### 4.3.2 Aspectos Avaliados
- **Nomes de Elementos**: Claros e descritivos?
- **Rela√ß√µes**: Bem definidas e compreens√≠veis?
- **Estrutura**: Organizada e l√≥gica?

#### 4.3.3 Processo de Avalia√ß√£o de Clareza

**Passo 4.3.3.1: Avalia√ß√£o por Especialistas**
- [ ] Preparar formul√°rio de avalia√ß√£o
- [ ] Para cada modelo, especialistas avaliam em escala Likert (1-5):
  - Nomes s√£o claros e descritivos? (1=confuso, 5=muito claro)
  - Rela√ß√µes s√£o bem definidas? (1=amb√≠guas, 5=muito claras)
  - Estrutura √© l√≥gica? (1=confusa, 5=muito l√≥gica)
- [ ] Coletar avalia√ß√µes de pelo menos 2 especialistas

**Passo 4.3.3.2: Calcular Score de Clareza**
```
Score de Clareza = M√©dia das avalia√ß√µes Likert
```

**Passo 4.3.3.3: An√°lise Qualitativa**
- [ ] Coletar coment√°rios qualitativos dos especialistas
- [ ] Identificar padr√µes de problemas de clareza
- [ ] Categorizar tipos de ambiguidade encontrados

**Passo 4.3.3.4: Implementar Processo**
- [ ] Criar formul√°rio em `/evaluation/expert_evaluations/clarity_form.md`
- [ ] Script para agregar avalia√ß√µes: `/evaluation/metrics/clarity_analyzer.py`
- [ ] Salvar resultados em `/evaluation/results/clarity_{scenario_id}.json`

### 4.4 M√©trica 3: Conformidade com iStar 2.0

#### 4.4.1 Defini√ß√£o
Mede a ader√™ncia do modelo √† especifica√ß√£o da nota√ß√£o iStar 2.0.

#### 4.4.2 Aspectos Avaliados
- **Estrutura JSON**: Conforme schema?
- **Tipos de Elementos**: Tipos v√°lidos?
- **Integridade Referencial**: IDs v√°lidos e refer√™ncias corretas?
- **Regras da Nota√ß√£o**: Segue regras do iStar 2.0?

#### 4.4.3 Processo de Avalia√ß√£o de Conformidade

**Passo 4.4.3.1: Valida√ß√£o Estrutural**
- [ ] Validar JSON contra schema (ver Se√ß√£o 5)
- [ ] Verificar tipos de elementos (agent, role, position, etc.)
- [ ] Verificar estrutura de cada elemento

**Passo 4.4.3.2: Valida√ß√£o de Integridade Referencial**
- [ ] Verificar que todos os IDs de atores em goals/tasks existem
- [ ] Verificar que depend√™ncias referenciam atores v√°lidos
- [ ] Verificar que dependums referenciam elementos v√°lidos

**Passo 4.4.3.3: Valida√ß√£o de Regras iStar 2.0**
- [ ] Goals devem ter ator associado
- [ ] Tasks devem ter ator associado
- [ ] Dependencies devem ter depender, dependee e dependum
- [ ] Softgoals devem ser distingu√≠veis de goals

**Passo 4.4.3.4: Calcular Score de Conformidade**
```
Conformidade = (Regras V√°lidas / Total de Regras) √ó 100%
```

**Passo 4.4.3.5: Implementar Script**
- [ ] Criar script `/evaluation/metrics/conformance_validator.py`
- [ ] Integrar com valida√ß√£o de JSON (Se√ß√£o 5)
- [ ] Salvar resultados em `/evaluation/results/conformance_{scenario_id}.json`

### 4.5 M√©trica 4: Qualidade das Perguntas (Apenas Interativo)

#### 4.5.1 Defini√ß√£o
Avalia a relev√¢ncia e utilidade das perguntas geradas na abordagem interativa.

#### 4.5.2 Aspectos Avaliados
- **Relev√¢ncia**: Pergunta √© relevante para o modelo?
- **Especificidade**: Pergunta √© espec√≠fica o suficiente?
- **Utilidade**: Resposta ajudou a melhorar o modelo?

#### 4.5.3 Processo de Avalia√ß√£o

**Passo 4.5.3.1: Categoriza√ß√£o de Perguntas**
- [ ] Categorizar cada pergunta por tipo:
  - Atores
  - Goals
  - Softgoals
  - Tasks
  - Dependencies
- [ ] Contar distribui√ß√£o por categoria

**Passo 4.5.3.2: Avalia√ß√£o por Especialistas**
- [ ] Especialistas avaliam cada pergunta em escala Likert (1-5):
  - Relev√¢ncia: (1=irrelevante, 5=muito relevante)
  - Especificidade: (1=vaga, 5=muito espec√≠fica)
  - Utilidade: (1=n√£o √∫til, 5=muito √∫til)

**Passo 4.5.3.3: An√°lise de Impacto**
- [ ] Comparar modelo gerado com e sem respostas
- [ ] Identificar quais perguntas levaram a melhorias no modelo
- [ ] Calcular correla√ß√£o entre qualidade da pergunta e melhoria do modelo

**Passo 4.5.3.4: Implementar Script**
- [ ] Criar script `/evaluation/metrics/question_quality_analyzer.py`
- [ ] Salvar resultados em `/evaluation/results/question_quality_{scenario_id}.json`

### 4.6 Consolida√ß√£o de M√©tricas

**Passo 4.6.1: Agregar Resultados**
- [ ] Para cada cen√°rio e abordagem, calcular todas as m√©tricas
- [ ] Criar tabela consolidada em `/evaluation/results/metrics_summary.csv`
- [ ] Incluir: cen√°rio, abordagem, completude, clareza, conformidade

**Passo 4.6.2: Visualiza√ß√µes**
- [ ] Gr√°fico de barras: Completude (baseline vs. interativo)
- [ ] Gr√°fico de barras: Clareza (baseline vs. interativo)
- [ ] Gr√°fico de barras: Conformidade (baseline vs. interativo)
- [ ] Heatmap: M√©tricas por cen√°rio
- [ ] Salvar em `/evaluation/visualizations/`

---

## 5. Processo para Validar o JSON

### 5.1 Objetivo

Garantir que os modelos gerados est√£o em formato JSON v√°lido e conforme ao schema iStar 2.0.

### 5.2 Passos de Valida√ß√£o

#### Passo 5.2.1: Criar Schema JSON

- [ ] Definir schema JSON Schema para iStar 2.0 (Pistar 2.0.0) em `/experiments/utils/istar_schema.json`
- [ ] **IMPORTANTE**: Usar estrutura exata definida em `ISTAR_2_0_JSON_STRUCTURE.md`
- [ ] Schema deve validar:
  - Estrutura raiz com campos: actors, orphans, dependencies, links, display, tool, istar, saveDate, diagram
  - `tool` deve ser exatamente `"pistar.2.0.0"`
  - `istar` deve ser exatamente `"2.0"`
  - Atores com estrutura: id, text, type (istar.Actor | istar.Agent | istar.Role), x, y, nodes
  - Nodes dentro de atores: id, text, type (istar.Goal | istar.Task | istar.Quality | istar.Resource), x, y
  - Dependencies: id, text, type, x, y, source, target
  - Links: id, type (v√°rios tipos v√°lidos), source, target, label (opcional)
- [ ] Consultar `ISTAR_2_0_JSON_STRUCTURE.md` para estrutura completa e exemplos

#### Passo 5.2.2: Valida√ß√£o de JSON B√°sico

- [ ] Criar fun√ß√£o para validar JSON sint√°tico:
  ```python
  def validate_json_syntax(json_string):
      try:
          json.loads(json_string)
          return True, None
      except json.JSONDecodeError as e:
          return False, str(e)
  ```
- [ ] Testar com modelos gerados
- [ ] Registrar erros de sintaxe

#### Passo 5.2.3: Extra√ß√£o de JSON da Resposta

- [ ] Criar fun√ß√£o para extrair JSON de respostas do LLM:
  - Procurar por code blocks (```json ... ```)
  - Procurar por objetos JSON diretos
  - Tentar parsing incremental se necess√°rio
- [ ] Implementar em `/experiments/utils/json_parser.py`

#### Passo 5.2.4: Valida√ß√£o contra Schema

- [ ] Usar biblioteca `jsonschema` para validar contra schema
- [ ] Criar fun√ß√£o:
  ```python
  def validate_istar_schema(json_data, schema):
      validator = jsonschema.Draft7Validator(schema)
      errors = list(validator.iter_errors(json_data))
      return len(errors) == 0, errors
  ```
- [ ] Implementar em `/experiments/utils/istar_validator.py`

#### Passo 5.2.5: Valida√ß√£o de Integridade Referencial

- [ ] Verificar que todos os IDs em `source` e `target` de dependencies referem a IDs de atores existentes
- [ ] Verificar que todos os IDs em `source` e `target` de links referem a IDs existentes (atores ou nodes)
- [ ] Verificar que todos os nodes est√£o dentro de atores (n√£o em orphans, a menos que necess√°rio)
- [ ] Verificar que todos os IDs s√£o √∫nicos em todo o modelo
- [ ] Implementar em `/experiments/utils/istar_validator.py`

#### Passo 5.2.6: Valida√ß√£o de Regras iStar 2.0

- [ ] Verificar tipos de atores: `istar.Actor`, `istar.Agent`, `istar.Role`
- [ ] Verificar tipos de nodes: `istar.Goal`, `istar.Task`, `istar.Quality`, `istar.Resource`
- [ ] Verificar tipos de links v√°lidos (OrRefinementLink, AndRefinementLink, DependencyLink, etc.)
- [ ] Verificar que `tool` √© exatamente `"pistar.2.0.0"`
- [ ] Verificar que `istar` √© exatamente `"2.0"`
- [ ] Verificar estrutura de dependencies (source, target, type, text)
- [ ] Implementar regras adicionais conforme `ISTAR_2_0_JSON_STRUCTURE.md`

#### Passo 5.2.7: Script de Valida√ß√£o Completo

- [ ] Criar script `/experiments/utils/validate_model.py` que:
  1. Carrega modelo JSON
  2. Valida sintaxe JSON
  3. Valida contra schema
  4. Valida integridade referencial
  5. Valida regras iStar 2.0
  6. Retorna relat√≥rio de valida√ß√£o
- [ ] Salvar relat√≥rios em `/evaluation/validation_reports/`

#### Passo 5.2.8: Classifica√ß√£o de Valida√ß√£o

- [ ] Classificar modelos como:
  - **V√°lido**: Passa todas as valida√ß√µes
  - **Parcialmente V√°lido**: Passa valida√ß√£o estrutural mas tem erros de integridade
  - **Inv√°lido**: Falha valida√ß√£o estrutural ou tem muitos erros
- [ ] Registrar classifica√ß√£o nos metadados do modelo

### 5.3 Tratamento de Erros

- [ ] Se JSON inv√°lido, tentar corre√ß√£o autom√°tica (se poss√≠vel)
- [ ] Registrar todos os erros encontrados
- [ ] Gerar relat√≥rio detalhado de erros
- [ ] Para modelos inv√°lidos, documentar tipo de erro mais comum

---

## 6. Processo para Comparar Baseline vs Interativo

### 6.1 Objetivo

Comparar sistematicamente os modelos gerados pelas abordagens baseline e interativa para identificar diferen√ßas e determinar qual produz melhores resultados.

### 6.2 Passos de Compara√ß√£o

#### Passo 6.2.1: Preparar Dados para Compara√ß√£o

- [ ] Para cada cen√°rio, carregar:
  - Modelo baseline: `/models/baseline/scenario_{id}_baseline_{timestamp}.json`
  - Modelo interativo: `/models/interactive/scenario_{id}_interactive_{timestamp}.json`
  - Modelo de refer√™ncia: `/models/reference/scenario_{id}_gold_standard.json`
- [ ] Verificar que modelos s√£o do mesmo cen√°rio
- [ ] Verificar que modelos foram gerados com mesma vers√£o de prompt

#### Passo 6.2.2: Compara√ß√£o Quantitativa

**Passo 6.2.2.1: Comparar Completude**
- [ ] Calcular completude de baseline vs. refer√™ncia
- [ ] Calcular completude de interativo vs. refer√™ncia
- [ ] Calcular diferen√ßa: `Completude_Interativo - Completude_Baseline`
- [ ] Testar signific√¢ncia estat√≠stica (se aplic√°vel)

**Passo 6.2.2.2: Comparar N√∫mero de Elementos**
- [ ] Contar elementos em cada modelo:
  - N√∫mero de atores
  - N√∫mero de goals
  - N√∫mero de softgoals
  - N√∫mero de tasks
  - N√∫mero de dependencies
- [ ] Calcular diferen√ßas absolutas e percentuais

**Passo 6.2.2.3: Comparar Conformidade**
- [ ] Comparar scores de conformidade
- [ ] Identificar tipos de erros mais comuns em cada abordagem
- [ ] Calcular taxa de modelos v√°lidos vs. inv√°lidos

#### Passo 6.2.3: Compara√ß√£o Qualitativa

**Passo 6.2.3.1: An√°lise de Elementos Faltantes**
- [ ] Identificar elementos no refer√™ncia que est√£o:
  - Presentes em ambos (baseline e interativo)
  - Presentes apenas no interativo
  - Presentes apenas no baseline
  - Ausentes em ambos
- [ ] Categorizar por tipo de elemento

**Passo 6.2.3.2: An√°lise de Elementos Incorretos**
- [ ] Identificar elementos incorretos ou mal definidos:
  - Nomes incorretos
  - Rela√ß√µes incorretas
  - Elementos inventados (n√£o no cen√°rio)
- [ ] Comparar frequ√™ncia entre abordagens

**Passo 6.2.3.3: An√°lise de Clareza**
- [ ] Comparar scores de clareza (avalia√ß√£o de especialistas)
- [ ] Identificar padr√µes de problemas de clareza
- [ ] Analisar coment√°rios qualitativos

#### Passo 6.2.4: An√°lise Estat√≠stica

**Passo 6.2.4.1: Estat√≠sticas Descritivas**
- [ ] Calcular para cada m√©trica:
  - M√©dia
  - Mediana
  - Desvio padr√£o
  - M√≠nimo e m√°ximo
- [ ] Separar por abordagem (baseline vs. interativo)

**Passo 6.2.4.2: Testes Estat√≠sticos (se aplic√°vel)**
- [ ] Se m√∫ltiplos cen√°rios, realizar:
  - Teste t de Student (se distribui√ß√£o normal)
  - Teste de Wilcoxon (se n√£o normal)
  - An√°lise de vari√¢ncia (ANOVA) se m√∫ltiplos fatores
- [ ] Calcular tamanho do efeito (Cohen's d)
- [ ] Interpretar signific√¢ncia estat√≠stica

#### Passo 6.2.5: An√°lise por Cen√°rio

- [ ] Para cada cen√°rio individualmente:
  - Comparar m√©tricas
  - Identificar padr√µes espec√≠ficos
  - Documentar observa√ß√µes
- [ ] Identificar se h√° diferen√ßas por dom√≠nio ou complexidade

#### Passo 6.2.6: An√°lise de Qualidade das Perguntas (Interativo)

- [ ] Analisar correla√ß√£o entre:
  - Qualidade das perguntas ‚Üí Melhoria no modelo
  - N√∫mero de perguntas ‚Üí Completude
  - Tipo de pergunta ‚Üí Tipo de elemento melhorado
- [ ] Identificar perguntas mais efetivas

#### Passo 6.2.7: Script de Compara√ß√£o Automatizado

- [ ] Criar script `/evaluation/comparison/compare_approaches.py` que:
  1. Carrega modelos baseline e interativo
  2. Calcula todas as m√©tricas
  3. Compara m√©tricas
  4. Gera relat√≥rio comparativo
  5. Cria visualiza√ß√µes
- [ ] Salvar resultados em `/evaluation/comparison/comparison_results.json`

#### Passo 6.2.8: Relat√≥rio Comparativo

- [ ] Criar relat√≥rio em `/evaluation/comparison/comparative_analysis.md` com:
  - Tabela comparativa de m√©tricas
  - An√°lise de diferen√ßas
  - Interpreta√ß√£o de resultados
  - Conclus√µes
- [ ] Incluir visualiza√ß√µes comparativas

### 6.3 Crit√©rios de Superioridade

Definir quando uma abordagem √© considerada "melhor":

- **Completude**: Interativo tem ‚â• 10% mais completude
- **Clareza**: Interativo tem score ‚â• 0.5 pontos maior (escala 1-5)
- **Conformidade**: Interativo tem ‚â• 5% mais conformidade
- **Consenso**: Especialistas preferem interativo em ‚â• 70% dos casos

### 6.4 Visualiza√ß√µes Comparativas

- [ ] Gr√°fico de barras lado a lado: M√©tricas (baseline vs. interativo)
- [ ] Gr√°fico de linha: Completude por tipo de elemento
- [ ] Heatmap: Diferen√ßas por cen√°rio
- [ ] Box plot: Distribui√ß√£o de m√©tricas
- [ ] Salvar em `/evaluation/visualizations/comparison/`

---

## 7. Checklist de Reprodutibilidade

### 7.1 Objetivo

Garantir que todos os experimentos podem ser reproduzidos exatamente, permitindo valida√ß√£o e extens√£o do trabalho.

### 7.2 Documenta√ß√£o de Ambiente

#### 7.2.1 Depend√™ncias de Software
- [ ] Criar `requirements.txt` com todas as depend√™ncias Python:
  ```
  openai==1.3.0
  anthropic==0.7.0
  jsonschema==4.20.0
  pandas==2.1.0
  matplotlib==3.8.0
  python-dotenv==1.0.0
  ```
- [ ] Especificar vers√µes exatas
- [ ] Documentar sistema operacional testado
- [ ] Documentar vers√£o do Python (ex: Python 3.11)

#### 7.2.2 Configura√ß√µes de API
- [ ] Criar `.env.example` com estrutura de vari√°veis:
  ```
  OPENAI_API_KEY=your_key_here
  ANTHROPIC_API_KEY=your_key_here
  ```
- [ ] Documentar como obter chaves de API
- [ ] **NUNCA** commitar chaves reais no reposit√≥rio

#### 7.2.3 Estrutura de Diret√≥rios
- [ ] Documentar estrutura completa de diret√≥rios
- [ ] Incluir no README principal
- [ ] Garantir que estrutura √© criada automaticamente (script de setup)

### 7.3 Versionamento de C√≥digo

#### 7.3.1 Controle de Vers√£o
- [ ] Usar Git para versionamento
- [ ] Criar tags para vers√µes importantes:
  - `v1.0-baseline-experiments`
  - `v1.0-interactive-experiments`
  - `v1.0-final-results`
- [ ] Documentar tags no README

#### 7.3.2 Commits Descritivos
- [ ] Commits devem descrever claramente mudan√ßas
- [ ] Usar conven√ß√£o de mensagens (ex: Conventional Commits)
- [ ] Incluir refer√™ncias a issues/tarefas

### 7.4 Versionamento de Dados

#### 7.4.1 Modelos Gerados
- [ ] Todos os modelos devem ter timestamps
- [ ] Metadados devem incluir:
  - Vers√£o do prompt usado
  - Modelo LLM usado
  - Par√¢metros (temperatura, tokens)
  - Hash do cen√°rio usado
- [ ] Manter hist√≥rico de modelos (n√£o sobrescrever)

#### 7.4.2 Prompts
- [ ] Versionar prompts (v1.0, v1.1, etc.)
- [ ] Documentar mudan√ßas entre vers√µes
- [ ] Salvar cada vers√£o em arquivo separado
- [ ] Manter log de efetividade por vers√£o

#### 7.4.3 Configura√ß√µes
- [ ] Versionar arquivos de configura√ß√£o
- [ ] Documentar prop√≥sito de cada configura√ß√£o
- [ ] Manter hist√≥rico de mudan√ßas

### 7.5 Sementes e Aleatoriedade

#### 7.5.1 Seeds para Reprodutibilidade
- [ ] Definir seed fixo para experimentos:
  ```python
  import random
  random.seed(42)
  ```
- [ ] Se usar LLM com op√ß√£o de seed, configurar seed fixo
- [ ] Documentar seed usado em cada experimento

#### 7.5.2 Par√¢metros do LLM
- [ ] Documentar todos os par√¢metros:
  - Modelo (ex: `gpt-4`, `claude-3-opus`)
  - Temperature (ex: `0.3`)
  - Max tokens (ex: `2000`)
  - Top-p (se usado)
  - Frequency penalty (se usado)
- [ ] Salvar em arquivo de configura√ß√£o versionado

### 7.6 Logs e Rastreabilidade

#### 7.6.1 Logs de Execu√ß√£o
- [ ] Registrar todas as chamadas de API:
  - Timestamp
  - Prompt enviado
  - Resposta recebida
  - Par√¢metros usados
  - Custo (se dispon√≠vel)
- [ ] Salvar logs em `/experiments/logs/`
- [ ] Formato estruturado (JSON ou CSV)

#### 7.6.2 Rastreabilidade de Modelos
- [ ] Cada modelo deve ser rastre√°vel at√©:
  - Cen√°rio usado
  - Prompt usado (vers√£o)
  - Configura√ß√£o do LLM
  - Timestamp de gera√ß√£o
- [ ] Manter √≠ndice de modelos em `/models/metadata/models_index.json`

### 7.7 Scripts de Reprodu√ß√£o

#### 7.7.1 Script de Setup
- [ ] Criar `setup.sh` ou `setup.py` que:
  - Cria estrutura de diret√≥rios
  - Instala depend√™ncias
  - Configura ambiente
- [ ] Testar em ambiente limpo

#### 7.7.2 Scripts de Execu√ß√£o
- [ ] Criar scripts que podem ser executados de forma idempotente:
  - `run_baseline_experiments.py`
  - `run_interactive_experiments.py`
  - `run_evaluation.py`
- [ ] Scripts devem poder ser executados m√∫ltiplas vezes sem efeitos colaterais
- [ ] Documentar ordem de execu√ß√£o

#### 7.7.3 Scripts de Valida√ß√£o
- [ ] Criar script que valida ambiente:
  - Verifica depend√™ncias instaladas
  - Verifica vari√°veis de ambiente
  - Verifica estrutura de diret√≥rios
  - Testa conex√£o com APIs

### 7.8 Documenta√ß√£o de Processo

#### 7.8.1 README Principal
- [ ] Incluir instru√ß√µes completas de setup
- [ ] Incluir instru√ß√µes de execu√ß√£o
- [ ] Incluir exemplos de uso
- [ ] Incluir troubleshooting comum

#### 7.8.2 Documenta√ß√£o de Experimentos
- [ ] Documentar cada experimento:
  - Objetivo
  - Configura√ß√£o
  - Resultados esperados
  - Como executar
- [ ] Manter em `/experiments/README.md`

#### 7.8.3 Documenta√ß√£o de Resultados
- [ ] Documentar como interpretar resultados
- [ ] Incluir exemplos de outputs esperados
- [ ] Documentar formato de arquivos de resultado

### 7.9 Valida√ß√£o de Reprodutibilidade

#### 7.9.1 Teste de Reprodu√ß√£o
- [ ] Executar experimentos em ambiente diferente:
  - M√°quina diferente
  - Sistema operacional diferente (se poss√≠vel)
  - Usu√°rio diferente
- [ ] Comparar resultados
- [ ] Documentar diferen√ßas (se houver)

#### 7.9.2 Checklist de Valida√ß√£o
Antes de considerar experimento reproduz√≠vel, verificar:
- [ ] Todas as depend√™ncias est√£o documentadas
- [ ] Todas as configura√ß√µes est√£o versionadas
- [ ] Todos os seeds est√£o definidos
- [ ] Todos os prompts est√£o versionados
- [ ] Logs est√£o completos
- [ ] Scripts podem ser executados sem interven√ß√£o manual
- [ ] Resultados s√£o id√™nticos (ou diferen√ßas s√£o documentadas)

### 7.10 Checklist Final de Reprodutibilidade

Antes de publicar ou compartilhar:

- [ ] **Ambiente**
  - [ ] `requirements.txt` completo e testado
  - [ ] `.env.example` documentado
  - [ ] Vers√µes de software documentadas

- [ ] **C√≥digo**
  - [ ] C√≥digo versionado no Git
  - [ ] Tags criadas para vers√µes importantes
  - [ ] README atualizado

- [ ] **Dados**
  - [ ] Modelos t√™m metadados completos
  - [ ] Prompts est√£o versionados
  - [ ] Configura√ß√µes est√£o versionadas

- [ ] **Execu√ß√£o**
  - [ ] Scripts podem ser executados automaticamente
  - [ ] Ordem de execu√ß√£o documentada
  - [ ] Logs s√£o gerados automaticamente

- [ ] **Valida√ß√£o**
  - [ ] Testado em ambiente limpo
  - [ ] Resultados s√£o reproduz√≠veis
  - [ ] Diferen√ßas documentadas (se houver)

- [ ] **Documenta√ß√£o**
  - [ ] README completo
  - [ ] Instru√ß√µes claras
  - [ ] Exemplos fornecidos
  - [ ] Troubleshooting documentado

---

## üìä Resumo do Plano de A√ß√£o

### Fases Principais

1. **Prepara√ß√£o** (Semanas 1-2)
   - Setup de cen√°rios
   - Desenvolvimento de prompts
   - Cria√ß√£o de modelos de refer√™ncia

2. **Execu√ß√£o Baseline** (Semana 3)
   - Gera√ß√£o de modelos zero-shot
   - Valida√ß√£o e armazenamento

3. **Execu√ß√£o Interativa** (Semanas 4-5)
   - Processo interativo completo
   - Gera√ß√£o de modelos finais

4. **Avalia√ß√£o** (Semanas 6-7)
   - C√°lculo de m√©tricas
   - Avalia√ß√£o de especialistas
   - Compara√ß√£o de abordagens

5. **An√°lise e Relat√≥rio** (Semana 8)
   - Consolida√ß√£o de resultados
   - Documenta√ß√£o final

### Entregas Principais

- Cen√°rios de requisitos validados
- Prompts versionados (baseline e interativo)
- Modelos gerados (baseline e interativo)
- Modelos de refer√™ncia (gold standard)
- M√©tricas de avalia√ß√£o calculadas
- Relat√≥rio comparativo
- Documenta√ß√£o completa para reprodu√ß√£o

---

## üìù Notas Finais

- Este plano deve ser seguido sequencialmente
- Cada passo deve ser completado antes de avan√ßar
- Documentar qualquer desvio do plano
- Manter registro de decis√µes e mudan√ßas
- Revisar e atualizar plano conforme necess√°rio

