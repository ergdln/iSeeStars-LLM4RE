# Como Usar a Interface Web Interativa

## ğŸš€ Iniciando o Servidor

### 1. Instalar DependÃªncias

```bash
pip3 install -r requirements.txt
```

### 2. Configurar API Key

Certifique-se de que a variÃ¡vel de ambiente `OPENAI_API_KEY` estÃ¡ configurada:

```bash
export OPENAI_API_KEY='sua-chave-api'
```

### 3. Iniciar o Servidor

```bash
python3 app.py
```

O servidor iniciarÃ¡ em: **http://localhost:5000**

### 4. Abrir no Navegador

Abra seu navegador e acesse: **http://localhost:5000**

## ğŸ“ Como Usar a Interface

### Etapa 1: Inserir CenÃ¡rio

1. Na primeira tela, digite ou cole o cenÃ¡rio de requisitos que deseja modelar
2. VocÃª pode clicar em "Carregar Exemplo" para ver um exemplo
3. Clique em "Iniciar ElicitaÃ§Ã£o"

### Etapa 2: Responder Perguntas

1. A IA gerarÃ¡ 5-8 perguntas de clarificaÃ§Ã£o
2. Responda cada pergunta no campo de texto correspondente
3. Todas as perguntas devem ser respondidas antes de continuar
4. Clique em "Gerar Modelo iStar" quando terminar

### Etapa 3: Visualizar e Baixar Modelo

1. O modelo iStar 2.0 serÃ¡ gerado automaticamente
2. VocÃª verÃ¡ estatÃ­sticas do modelo (atores, nodes, dependencies, links)
3. O JSON completo serÃ¡ exibido
4. VocÃª pode:
   - **Copiar** o JSON para a Ã¡rea de transferÃªncia
   - **Baixar** o JSON como arquivo
   - **Validar** o JSON
   - **Iniciar Nova SessÃ£o**

## ğŸ”§ ConfiguraÃ§Ãµes Opcionais

### Modelo do LLM

Por padrÃ£o, o sistema usa `gpt-4o-mini`. Para usar outro modelo:

```bash
export OPENAI_MODEL='gpt-4'
```

### Porta do Servidor

Para mudar a porta (padrÃ£o: 5000), edite o arquivo `app.py`:

```python
app.run(debug=True, host='0.0.0.0', port=5000)  # Mude 5000 para outra porta
```

## âš ï¸ SoluÃ§Ã£o de Problemas

### Erro: "OPENAI_API_KEY nÃ£o configurada"

Certifique-se de exportar a variÃ¡vel de ambiente antes de iniciar o servidor:

```bash
export OPENAI_API_KEY='sua-chave'
python3 app.py
```

### Erro: "Prompt nÃ£o encontrado"

Certifique-se de que os arquivos de prompt estÃ£o no diretÃ³rio `prompts/`:
- `prompts/interactive_master.txt`
- `prompts/final_json_generation.txt`

### Erro ao gerar perguntas ou modelo

- Verifique sua conexÃ£o com a internet
- Verifique se sua API key tem crÃ©ditos disponÃ­veis
- Verifique os logs do servidor para mais detalhes

## ğŸ“‹ Estrutura de Arquivos

```
iSeeStars-LLM4RE/
â”œâ”€â”€ app.py                 # Servidor Flask
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Interface HTML
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css         # Estilos CSS
â”‚   â””â”€â”€ script.js          # JavaScript frontend
â””â”€â”€ prompts/
    â”œâ”€â”€ interactive_master.txt
    â””â”€â”€ final_json_generation.txt
```

## ğŸ¯ Fluxo Completo

1. **UsuÃ¡rio insere cenÃ¡rio** â†’ Interface envia para `/api/start`
2. **IA gera perguntas** â†’ Interface chama `/api/generate-questions`
3. **UsuÃ¡rio responde** â†’ Interface envia para `/api/submit-answer`
4. **IA gera modelo** â†’ Interface chama `/api/generate-model`
5. **JSON corrigido** â†’ Aplicado automaticamente pelo servidor
6. **UsuÃ¡rio baixa JSON** â†’ Pronto para usar no iStar

## ğŸ’¡ Dicas

- **CenÃ¡rios detalhados** geram melhores modelos
- **Responda todas as perguntas** completamente para melhores resultados
- **Valide o JSON** antes de usar no iStar
- O JSON gerado jÃ¡ estÃ¡ corrigido e pronto para uso

